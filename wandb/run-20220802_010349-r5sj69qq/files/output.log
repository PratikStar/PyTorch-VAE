MusicVAEFlat(
  (encoder): Sequential(
    (0): Sequential(
      (conv2d_0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))
      (batchNorm2d_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (leakyReLU_0): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (conv2d_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))
      (batchNorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (leakyReLU_1): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (conv2d_2): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))
      (batchNorm2d_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (leakyReLU_2): LeakyReLU(negative_slope=0.01)
    )
  )
  (fc_mu): Sequential(
    (conv2d_fc_mu): Conv2d(32, 16, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))
    (batchNorm2d_fc_mu): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (leakyReLU_fc_mu): LeakyReLU(negative_slope=0.01)
  )
  (fc_var): Sequential(
    (conv2d_fc_var): Conv2d(32, 16, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))
    (batchNorm2d_fc_var): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (leakyReLU_fc_var): LeakyReLU(negative_slope=0.01)
  )
  (decoder): Sequential(
    (0): Sequential(
      (convTranspose2d_1): ConvTranspose2d(16, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))
      (batchnorm2d_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (leakyReLU_1): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (convTranspose2d_2): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))
      (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (leakyReLU_2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (convTranspose2d_3): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))
      (batchnorm2d_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (leakyReLU_3): LeakyReLU(negative_slope=0.01)
    )
  )
  (final_layer): Sequential(
    (final_convTranspose2d): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))
    (final_batchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (final_leakyReLU): LeakyReLU(negative_slope=0.01)
    (final_Conv2d): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (final_tanH): Sigmoid()
  )
)
======= Training MusicVAEFlat =======
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Global seed set to 1265
initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=gloo
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
Traceback (most recent call last):
  File "run-timbre-vae.py", line 68, in <module>
    trainer.fit(vae, datamodule=data)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 737, in fit
    self._call_and_handle_interrupt(
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 682, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 772, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1140, in _run
    self.accelerator.setup(self)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu.py", line 35, in setup
    return super().setup(trainer)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 93, in setup
    self.setup_optimizers(trainer)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 351, in setup_optimizers
    optimizers, lr_schedulers, optimizer_frequencies = self.training_type_plugin.init_optimizers(
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 245, in init_optimizers
    return trainer.init_optimizers(model)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/optimizers.py", line 35, in init_optimizers
    optim_conf = self.call_hook("configure_optimizers", pl_module=pl_module)
  File "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1496, in call_hook
    output = model_fx(*args, **kwargs)
  File "/Users/pratik/repos/TimbreSpace/experiment_music.py", line 83, in configure_optimizers
    lr=self.config['exp_params']['LR'],
  File "/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 128, in __getitem__
    return self._items[key]
KeyError: 'exp_params'